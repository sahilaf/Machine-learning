{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSuU6lq6IK9FeQCjoGJJSK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilaf/Machine-learning/blob/main/Exploratory_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "424e7658"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17c6811f"
      },
      "source": [
        "**EDA Steps**\n",
        "\n",
        "Here's a structured approach to Exploratory Data Analysis:\n",
        "\n",
        "### Step 1: Understand the Data\n",
        "- View initial rows and columns: Use `.head()`, `.tail()`, `.shape`.\n",
        "- Examine data types and non-null values: Use `.info()`.\n",
        "- Identify columns and their data types.\n",
        "\n",
        "### Step 2: Summary Statistics\n",
        "- Calculate descriptive statistics: Use `.describe()`. This includes mean, median, mode, standard deviation, min, max, and quartiles for numerical data.\n",
        "- Understand the spread and central tendency of the data.\n",
        "\n",
        "### Step 3: Value Counts\n",
        "- Check unique values and their frequencies in columns: Use `.value_counts()`.\n",
        "- Identify potential duplicates or inconsistencies in categorical data.\n",
        "- Example: `df['column_name'].value_counts()`\n",
        "\n",
        "### Step 4: Missing Value Analysis\n",
        "- Identify where data is missing: Use `.isnull()`.\n",
        "- Calculate the percentage of missing data per column: Use `.isnull().sum() / len(df) * 100`.\n",
        "- Understand the extent and location of gaps in the dataset.\n",
        "\n",
        "### Step 5: Visualizations\n",
        "- **Histograms**: Show the distribution of numerical variables. Example: `df['numerical_column'].hist()`\n",
        "- **Boxplots**: Identify outliers and understand the spread of numerical data. Example: `df.boxplot(column='numerical_column')`\n",
        "- **Bar plots**: Compare categories. Example: `df['categorical_column'].value_counts().plot(kind='bar')`\n",
        "- **Correlation Heatmaps**: Visualize linear relationships between numerical features. Example: `sns.heatmap(df.corr(), annot=True)` (Requires importing `seaborn`)\n",
        "- **Scatter Plots**: Explore bivariate relationships between two numerical variables. Example: `df.plot(kind='scatter', x='column1', y='column2')`\n",
        "\n",
        "### Step 6: Target Variable Exploration\n",
        "- Analyze the distribution of the target variable.\n",
        "- Explore how the target variable relates to other features using visualizations and summary statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n"
      ],
      "metadata": {
        "id": "DtRI6ObtHxqS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f73dd2fe"
      },
      "source": [
        "Strategies to handle common data cleaning issues:\n",
        "\n",
        "### 1. Handle Missing Values\n",
        "\n",
        "- **Drop missing rows/columns**: If the number of missing values is very small or the column is not essential. Use `df.dropna()`.\n",
        "- **Impute missing values**:\n",
        "    - **Numerical data**: Fill with the mean or median. Example: `df['numerical_column'].fillna(df['numerical_column'].mean())`\n",
        "    - **Categorical data**: Fill with the mode. Example: `df['categorical_column'].fillna(df['categorical_column'].mode()[0])`\n",
        "    - **Advanced methods**: Consider using linear regression, KNN, or interpolation for more sophisticated imputation (for future learning).\n",
        "\n",
        "### 2. Remove Duplicates\n",
        "- Detect and drop exact duplicate rows: Use `df.drop_duplicates()`.\n",
        "\n",
        "### 3. Fix Data Types\n",
        "- Convert columns to the correct data types (e.g., from object to datetime or numerical). Example: `pd.to_datetime(df['date_column'])` or `df['numerical_column'].astype(int)`\n",
        "\n",
        "### 4. Handle Inconsistent Categories\n",
        "- Clean up categorical values to ensure uniformity (e.g., 'USA', 'U.S.A.', and 'United States' should be consistent). This might involve using string manipulation methods.\n",
        "\n",
        "### 5. Detect and Handle Outliers\n",
        "- **Detection**: Use visualizations like boxplots, or statistical methods like the Interquartile Range (IQR) or Z-score.\n",
        "- **Handling**: Remove outliers (if they are errors) or cap them at a certain value. Example (capping using IQR): `Q1 = df['column'].quantile(0.25)`, `Q3 = df['column'].quantile(0.75)`, `IQR = Q3 - Q1`, `upper_bound = Q3 + 1.5 * IQR`, `df['column'] = df['column'].clip(upper=upper_bound)`\n",
        "\n",
        "### 6. Fix Logical or Domain Errors\n",
        "- Address values that are incorrect based on domain knowledge (e.g., a negative age, or a purchase date before the product was released). This often requires conditional logic to identify and correct these values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "554e2ebf"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3dda14c"
      },
      "source": [
        "\n",
        "Data preprocessing is a crucial step to prepare your data for machine learning models.\n",
        "\n",
        "### 1. Encoding Categorical Variables\n",
        "- Convert text labels into numerical representations that machine learning algorithms can understand.\n",
        "\n",
        "#### Methods:\n",
        "1. **Label Encoding (Ordinal)**:\n",
        "   - Good for ordered categorical variables where there is a natural ranking (e.g., \"low\", \"medium\", \"high\"). Assigns a unique integer to each category.\n",
        "   - Example: Encoding 'size' column: `from sklearn.preprocessing import LabelEncoder`, `le = LabelEncoder()`, `df['size_encoded'] = le.fit_transform(df['size'])`\n",
        "\n",
        "2. **One-Hot Encoding (Nominal)**:\n",
        "   - For non-ordered categorical variables where there is no inherent ranking (e.g., 'region', 'color'). Creates new binary columns for each category.\n",
        "   - Example: Encoding 'color' column: `pd.get_dummies(df['color'])` or `from sklearn.preprocessing import OneHotEncoder`, `ohe = OneHotEncoder()`, `encoded_data = ohe.fit_transform(df[['color']])`\n",
        "\n",
        "### 2. Feature Transformation\n",
        "- Used to handle skewed data distributions (e.g., right-skewed or left-skewed data) or to meet the assumptions of certain models.\n",
        "- Common transformations include logarithmic transformation, square root transformation, or Box-Cox transformation.\n",
        "- Example (Log transform): `import numpy as np`, `df['skewed_column_log'] = np.log(df['skewed_column'])`\n",
        "\n",
        "### 3. Feature Scaling\n",
        "- Standardize the range of independent features. This is important for algorithms that are sensitive to the scale of the input data (e.g., gradient descent-based algorithms, KNN, SVMs).\n",
        "- Converts all features to a similar scale, often between 0 and 1 or with a mean of 0 and standard deviation of 1.\n",
        "\n",
        "#### Methods:\n",
        "1. **Min-Max Scaling**: Scales features to a fixed range, usually 0 to 1. Example: `from sklearn.preprocessing import MinMaxScaler`, `scaler = MinMaxScaler()`, `df[['column1', 'column2']] = scaler.fit_transform(df[['column1', 'column2']])`\n",
        "\n",
        "\n",
        "2. **Standardization (Z-score scaling)**: Scales features to have a mean of 0 and a standard deviation of 1. Example: `from sklearn.preprocessing import StandardScaler`, `scaler = StandardScaler()`, `df[['column1', 'column2']] = scaler.fit_transform(df[['column1', 'column2']])`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "95fLBIAQIRiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating new features or transforming existing ones to expose useful patterns that ML models can learn from.\n",
        "\n",
        "## Common techniques:\n",
        "- Mathematical Combinations\n",
        "- Target-Based flags\n",
        "- Binning\n",
        "- Time-based features"
      ],
      "metadata": {
        "id": "iym6o6VUIWhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection"
      ],
      "metadata": {
        "id": "EzfrZiflJUJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting the most useful features and removing the rest.\n",
        "\n",
        "**Why is it important?**\n",
        "- Reduces noise and overfitting\n",
        "- Speeds up traning\n",
        "- Impoves accuracy\n",
        "- Makes model interpretation easier\n",
        "\n",
        "## Methods:\n",
        "### **1. Filter methods (Pure Stat)**\n",
        "- Correlation Matrix -> remove highly correlated features\n",
        "- Chi-square test (categorical vs cetagorical)\n",
        "- Anova F-test (numerical vs categorical target)\n",
        "\n",
        "### **2. Embedded methods (Selection build into the model)**\n",
        "- Lasso Regression -> Shrinks coefficients to 0\n",
        "- Tree-based models (random forest,xgboost) -> feature importance scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "KBIsAaqEJY1u"
      }
    }
  ]
}